# RTL optimization specific ReMA training configuration
# Modified from original ppo_trainer.yaml, targeting RTL code optimization tasks

data:
  tokenizer: null
  # RTL optimization data paths - users need to modify according to actual situation
  train_files: data/rtl_optimization/train.parquet
  val_files: data/rtl_optimization/val.parquet
  prompt_key: question  # RTL design task as prompt (ReMA standard)
  response_key: ground_truth  # Reference optimized code as response
  max_prompt_length: 4096  # RTL code is usually longer
  max_response_length: 4096  # Optimized code length
  train_batch_size: 64  # Adjust according to GPU memory
  val_batch_size: null
  return_raw_input_ids: False
  return_raw_chat: False
  shuffle: True
  filter_overlong_prompts: True  # Filter overly long RTL code
  truncation: left  # Truncate from left, preserve code body

  # RTL special processing
  code_start_token: "```verilog"
  code_end_token: "```"
  preserve_code_structure: True
  task_type: rtl_generation  # Task type for proper prompt handling

# Model configuration - using Verilog-specific models
actor_rollout_ref:
  hybrid_engine: True
  model:
    # Recommended to use DeepSeek-Coder or other Verilog-specific models
    path: deepseek-ai/deepseek-coder-6.7b-instruct
    # Optional other Verilog models:
    # path: henryen/OriGen_Fix
    # path: Nellyw888/VeriReason-Qwen2.5-7b-RTLCoder-Verilog-GRPO-reasoning-tb
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: True
    use_remove_padding: False

  ref_model:
    path: ${actor_rollout_ref.model.path}
    external_lib: ${actor_rollout_ref.model.external_lib}
    override_config: ${actor_rollout_ref.model.override_config}
    enable_gradient_checkpointing: ${actor_rollout_ref.model.enable_gradient_checkpointing}
    use_remove_padding: ${actor_rollout_ref.model.use_remove_padding}

  # ReMA feature configuration
  actor:
    optim:
      lr: 5e-6  # Smaller learning rate, suitable for code optimization tasks
      beta1: 0.9
      beta2: 0.95
      eps: 1.0e-8
      weight_decay: 0.01
    ppo_mini_batch_size: 256
    ppo_micro_batch_size_per_gpu: 8
    gradient_checkpointing: false
    flash_attention_2: false

    # ReMA core parameters
    clip_mode: turn  # turn-level clipping for multi-agent
    agg_mode: trajectory  # trajectory aggregation
    kl_ctrl:
      type: adaptive
      target: 6.0
      horizon: 10000

    # RTL-specific training parameters
    max_new_tokens_per_turn: 2048  # Maximum tokens generated per turn
    temperature: 0.7  # Moderate randomness
    top_p: 0.9

  ref:
    log_prob_micro_batch_size_per_gpu: 16

  rollout:
    log_prob_micro_batch_size_per_gpu: 16
    tensor_model_parallel_size: 1
    max_num_batched_tokens: 4096

    # Multi-turn dialogue configuration (supporting multi-step optimization)
    max_num_turns: 15  # Support multi-turn optimization interaction
    n: 8  # rollout count
    stop_when_truncated: True

    # RTL special configuration
    enable_code_validation: True  # Enable code validation
    preserve_module_structure: True  # Preserve module structure

# Trainer configuration
trainer:
  project_name: rtl_optimization
  experiment_name: rtl_rema_experiment
  save_freq: 10
  test_freq: 5
  total_epochs: 50
  total_training_steps: 2000

  # Validation configuration
  val_before_train: True
  val_only: False
  save_val_generations: True
  save_train_generations: True

  # Model saving
  remove_previous_ckpt_in_save: False  # Keep historical checkpoints

  # Logging configuration
  logger: [console, wandb]

# RTL-specific reward function configuration
# Uses the integrated reward function in utils/reward_score/rtl_optimization.py
# No custom_reward_function needed as it's integrated into ReMA's default system

# Reward function parameters
reward_model:
  reward_manager: rema
  mask_unfinished_reward: True

  # RTL validation configuration
  verification_tools:
    enable_verilator: True
    enable_yosys: True
    enable_iverilog: True

  # Reward weights
  syntax_weight: 0.4    # Syntax correctness weight
  synthesis_weight: 0.4  # Synthesis success weight
  ppa_weight: 0.2       # PPA improvement weight

# Algorithm configuration
algorithm:
  adv_estimator: grpo  # ReMA recommended advantage estimator

  # Filter configuration
  filter_groups:
    enable: True
    min_reward_threshold: 0.1  # Minimum reward threshold
    max_response_length: 8192   # Maximum response length

  # RTL-specific filters
  rtl_filters:
    require_module_structure: True  # Require module structure
    require_endmodule: True         # Require endmodule
    filter_syntax_errors: True     # Filter syntax errors

# Switch Agent configuration (if using separated trainer)
switch_agent:
  enable: False  # Currently using shared parameter mode
  freq: 10
  model_paths: null

# Hardware configuration
hardware:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  distributed_backend: nccl

# Debug configuration
debug:
  log_level: INFO
  save_intermediate_codes: True
  verbose_validation: True
  profile_reward_calculation: False