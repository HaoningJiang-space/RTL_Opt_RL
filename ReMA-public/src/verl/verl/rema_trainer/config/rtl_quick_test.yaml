# RTL optimization quick test configuration
# For rapid system function verification, using smaller parameters

data:
  tokenizer: null
  # Using generated sample data for testing
  train_files: data/rtl_test/sample_train.parquet
  val_files: data/rtl_test/sample_val.parquet
  prompt_key: original_code
  response_key: optimized_code
  max_prompt_length: 2048  # Shorter length for testing
  max_response_length: 2048
  train_batch_size: 16  # Small batch testing
  val_batch_size: null
  return_raw_input_ids: False
  return_raw_chat: False
  shuffle: True
  filter_overlong_prompts: True
  truncation: left

  # Test mode flags
  test_mode: True
  generate_sample_data: True  # Automatically generate sample data
  sample_data_size: 50

actor_rollout_ref:
  hybrid_engine: True
  model:
    # Use smaller or locally available model for testing
    path: deepseek-ai/deepseek-coder-6.7b-instruct
    external_lib: null
    override_config: {}
    enable_gradient_checkpointing: False  # Disabled to simplify debugging
    use_remove_padding: False

  ref_model:
    path: ${actor_rollout_ref.model.path}
    external_lib: ${actor_rollout_ref.model.external_lib}
    override_config: ${actor_rollout_ref.model.override_config}
    enable_gradient_checkpointing: ${actor_rollout_ref.model.enable_gradient_checkpointing}
    use_remove_padding: ${actor_rollout_ref.model.use_remove_padding}

  actor:
    optim:
      lr: 1e-4  # Larger learning rate for quick testing
      beta1: 0.9
      beta2: 0.95
      eps: 1.0e-8
      weight_decay: 0.01
    ppo_mini_batch_size: 64
    ppo_micro_batch_size_per_gpu: 4
    gradient_checkpointing: false
    flash_attention_2: false

    clip_mode: turn
    agg_mode: trajectory
    kl_ctrl:
      type: adaptive
      target: 6.0
      horizon: 1000  # Shorter horizon

    max_new_tokens_per_turn: 1024  # Fewer generation tokens
    temperature: 0.8
    top_p: 0.9

  ref:
    log_prob_micro_batch_size_per_gpu: 8

  rollout:
    log_prob_micro_batch_size_per_gpu: 8
    tensor_model_parallel_size: 1
    max_num_batched_tokens: 2048

    max_num_turns: 5  # Fewer turns for quick testing
    n: 4  # Fewer rollout count
    stop_when_truncated: True

    enable_code_validation: True
    preserve_module_structure: True

# Trainer configuration - quick test parameters
trainer:
  project_name: rtl_quick_test
  experiment_name: rtl_test_run
  save_freq: 2  # Frequent saving for debugging
  test_freq: 1  # Test every epoch
  total_epochs: 5  # Only run 5 epochs
  total_training_steps: 50  # Fewer training steps

  val_before_train: True
  val_only: False
  save_val_generations: True
  save_train_generations: True

  remove_previous_ckpt_in_save: False

  logger: [console]  # Only use console logging, no wandb

# Custom reward function
custom_reward_function:
  path: src/verl/verl/rema_trainer/rtl_reward_manager.py
  name: rtl_reward_function

# Reward model configuration
reward_model:
  reward_manager: rema
  mask_unfinished_reward: True

  # Verification configuration in test mode
  verification_tools:
    enable_verilator: True
    enable_yosys: False  # Disable slower synthesis checks
    enable_iverilog: True

  # Test weights
  syntax_weight: 0.6  # Focus more on syntax correctness
  synthesis_weight: 0.0  # Disable synthesis checks
  ppa_weight: 0.4

# Algorithm configuration
algorithm:
  adv_estimator: grpo

  filter_groups:
    enable: True
    min_reward_threshold: 0.05  # Lower threshold for testing
    max_response_length: 4096

  rtl_filters:
    require_module_structure: True
    require_endmodule: True
    filter_syntax_errors: False  # Allow some syntax errors during testing

# Switch Agent configuration
switch_agent:
  enable: False

# Hardware configuration
hardware:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  distributed_backend: nccl

# Debug configuration
debug:
  log_level: DEBUG  # Detailed logging
  save_intermediate_codes: True
  verbose_validation: True
  profile_reward_calculation: True