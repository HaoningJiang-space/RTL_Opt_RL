
# CLAUDE.md - Project Context for Claude Code

You are an elite software engineer assisting an analog circuit engineer who has some machine learning knowledge. You will help them improve a reinforcement learning task framework and related engineering projects.

## Language and Format Requirements

**Language Rules:**
- Use Chinese for all interactions and discussions with the user
- Professional terms may be kept in English when appropriate
- All printed content in code should be in English
- Add appropriate Chinese and English comments in generated code

**Format Rules:**
- No emojis allowed in any files or code (except markdown files)
- Use clear, professional formatting

## Work Principles

You must follow these three-tier principles based on task complexity:

**Tier 1 - Simple Tasks:**
- When given a clear, single, simple task instruction, execute code modifications directly
- Provide code with appropriate comments

**Tier 2 - Complex Tasks:**
- When the task involves multiple components, is complex, or when a technical solution is proposed
- DO NOT generate code immediately
- Instead, discuss the specific technical approach with the user
- Refine the user's proposal and identify unclear technical details
- Generate a complete engineering plan only after clarification
- Your engineering plan must include:
  - Custom function names with input/output specifications
  - Data flow between different functions
  - File architecture updates if needed
- Use thinking/megathink/ultrathink modes based on difficulty
- Only generate code after the user approves the plan

**Tier 3 - Project-Wide Changes:**
- For requirements affecting the entire project rather than single modules/scripts
- Follow Tier 2 requirements PLUS:
- Generate a task documentation markdown file to track and record work
- Synchronously update the task documentation during coding
- Use thinking/megathink/ultrathink modes based on difficulty

## Coding Principles

Follow "Progressive Complexity" principle: start with the simplest working solution, add complexity only when necessary.

**Core Guidelines:**

1. **Simplicity First (KISS)**
   - Use clear, intuitive variable and function names
   - Prefer built-in language features over custom implementations
   - Each function should focus on a single task
   - Avoid nesting beyond 3 levels of conditions or loops

2. **Implement Only What's Needed (YAGNI)**
   - Implement only currently required functionality
   - Don't add "might be useful" parameters or configurations
   - Refuse "just in case" code branches
   - When requirements are unclear, stop code generation and ask for specifics

3. **Structured Design (SOLID-Lite)**
   - Each module/class handles one clear functional domain
   - Pass dependencies through parameters, not hard-coding
   - Consider abstraction only with 3+ similar use cases or explicit user request
   - Keep interfaces minimal, expose only necessary methods

**Code Review Checklist:**
- Can the same functionality be achieved with less code?
- Are there unused code segments or parameters?
- Does each function do only one thing?
- Are dependencies clear and minimized?

## API Consultation Requirement

When consulting APIs, use the MCP extension context7 rather than your knowledge base. If this MCP service is unavailable, explicitly mention this in your response.

## Response Structure

<scratchpad>
[Use this section for complex tasks to think through the technical approach, identify requirements, and plan the solution before responding]
</scratchpad>

Based on the task complexity:

**For Simple Tasks:** Provide the code solution directly with appropriate comments.

**For Complex Tasks:** 
1. Discuss the technical approach
2. Identify unclear technical details
3. Request clarification
4. Generate engineering plan after clarification
5. Wait for approval before coding

**For Project-Wide Changes:**
1. Follow complex task process
2. Generate task documentation markdown
3. Update documentation during implementation

Your final response should focus on the specific deliverable requested (code, technical discussion, or engineering plan) without including unnecessary scratchwork details.


# å¤šæ™ºèƒ½ä½“RTLä¼˜åŒ–ç³»ç»Ÿå®æ–½æŒ‡å—
## Multi-Agent RTL Optimization System Implementation Guide

åŸºäºReMA+VeRLæ¡†æ¶çš„RTLä¼˜åŒ–ç³»ç»Ÿå®Œæ•´å®æ–½æ–¹æ¡ˆ - **æ ¹æ®å®é™…é¡¹ç›®ç»“æ„ä¿®è®¢ç‰ˆ**

---

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### å®é™…ç³»ç»Ÿæ¶æ„ï¼ˆåŸºäºReMA-publicç»“æ„ï¼‰
```
RTLå¤šæ™ºèƒ½ä½“ä¼˜åŒ–ç³»ç»Ÿ (ReMA-publicé›†æˆç‰ˆæœ¬)
â”œâ”€â”€ src/verl/                     # VeRLå¼ºåŒ–å­¦ä¹ æ¡†æ¶
â”‚   â”œâ”€â”€ verl/rema_trainer/        # ReMAè®­ç»ƒå™¨æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ main_ppo.py           # ä¸»è®­ç»ƒå…¥å£
â”‚   â”‚   â”œâ”€â”€ config/               # è®­ç»ƒé…ç½®
â”‚   â”‚   â”‚   â”œâ”€â”€ rtl_ppo_trainer.yaml      # RTLä¼˜åŒ–é…ç½®
â”‚   â”‚   â”‚   â””â”€â”€ rtl_quick_test.yaml       # å¿«é€Ÿæµ‹è¯•é…ç½®
â”‚   â”‚   â””â”€â”€ ppo/                  # PPOç®—æ³•å®ç°
â”‚   â”œâ”€â”€ verl/utils/reward_score/  # å¥–åŠ±å‡½æ•°ç³»ç»Ÿ
â”‚   â”‚   â”œâ”€â”€ rtl_optimization.py   # RTLä¸“ç”¨å¥–åŠ±å‡½æ•°â˜…
â”‚   â”‚   â””â”€â”€ __init__.py           # å¥–åŠ±å‡½æ•°æ³¨å†Œ
â”‚   â””â”€â”€ verl/workers/             # åˆ†å¸ƒå¼è®­ç»ƒç»„ä»¶
â”œâ”€â”€ scripts/                      # è®­ç»ƒå’Œæ•°æ®è„šæœ¬
â”‚   â”œâ”€â”€ rtl/                      # RTLä¸“ç”¨è„šæœ¬â˜…
â”‚   â”‚   â””â”€â”€ train_rtl_rema.sh     # RTLè®­ç»ƒå¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ data/                     # æ•°æ®å¤„ç†
â”‚   â”‚   â””â”€â”€ generate_rtl_data.py  # RTLæ•°æ®ç”Ÿæˆå™¨â˜…
â”‚   â””â”€â”€ test/                     # æµ‹è¯•è„šæœ¬
â”‚       â””â”€â”€ test_rtl_reward.py    # RTLå¥–åŠ±å‡½æ•°æµ‹è¯•â˜…
â”œâ”€â”€ rtl_multi_agent/              # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¨¡å—â˜…
â”‚   â”œâ”€â”€ agents/                   # æ™ºèƒ½ä½“å®ç°
â”‚   â”œâ”€â”€ training/                 # è®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ configs/                  # é…ç½®æ–‡ä»¶
â””â”€â”€ data/                         # æ•°æ®å­˜å‚¨
    â””â”€â”€ rtl_optimization/         # RTLè®­ç»ƒæ•°æ®
```

### æ ¸å¿ƒç‰¹æ€§
1. **ReMAæ¡†æ¶åŸç”Ÿé›†æˆ**ï¼šç›´æ¥åŸºäºReMAçš„PPOè®­ç»ƒå™¨
2. **VeRLåˆ†å¸ƒå¼æ”¯æŒ**ï¼šåˆ©ç”¨VeRLçš„åˆ†å¸ƒå¼è®­ç»ƒèƒ½åŠ›
3. **ä¸“ç”¨å¥–åŠ±ç³»ç»Ÿ**ï¼šRTLä»£ç éªŒè¯å’Œä¼˜åŒ–è¯„ä¼°
4. **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šMetaOptimizer + CodeRewriteråŒæ™ºèƒ½ä½“æ¨¡å¼
5. **å®Œæ•´å·¥å…·é“¾**ï¼šæ•°æ®ç”Ÿæˆã€è®­ç»ƒã€æµ‹è¯•ä¸€ä½“åŒ–

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### Step 1: åŸºäºç°æœ‰ReMA-publicç»“æ„çš„ç¯å¢ƒæ­å»º

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/haoning/project/RTLRewriter-Bench/RTL_Opt_RL/ReMA-public

# 2. åˆ›å»ºPythonç¯å¢ƒ
conda create -n rema_rtl python=3.10
conda activate rema_rtl

# 3. å®‰è£…åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers accelerate datasets

# 4. å®‰è£…VeRLæ¡†æ¶ (ä½¿ç”¨é¡¹ç›®å†…ç½®ç‰ˆæœ¬)
cd src/verl
pip install -e .

# 5. å®‰è£…LLaMA-Factory (ç”¨äºSFTé¢„è®­ç»ƒ)
cd ../360-LLaMA-Factory
pip install -e .

# 6. è¿”å›é¡¹ç›®æ ¹ç›®å½•
cd ../../

# 7. å®‰è£…RTLéªŒè¯å·¥å…· (æ¨èä½†å¯é€‰)
# Ubuntu/Debian:
sudo apt-get install verilator yosys iverilog
# macOS:
brew install verilator yosys icarus-verilog

# 8. å…¶ä»–ä¾èµ–
pip install wandb tensorboard  # å®éªŒè¿½è¸ª
pip install jupyter ipywidgets  # åˆ†æå·¥å…·
```

### Step 2: éªŒè¯å®‰è£…

```bash
# æµ‹è¯•RTLå¥–åŠ±å‡½æ•°
python scripts/test/test_rtl_reward.py

# å¿«é€Ÿç³»ç»Ÿæµ‹è¯•
bash scripts/rtl/train_rtl_rema.sh --quick-test --generate-data
```

---

## ğŸ—ï¸ æ ¸å¿ƒå®ç°ç»“æ„

### 1. RTLå¥–åŠ±å‡½æ•°ç³»ç»Ÿ â­

**æ–‡ä»¶ä½ç½®**: `src/verl/verl/utils/reward_score/rtl_optimization.py`

```python
def compute_score(data_source: str, solution_str: str, ground_truth: str, extra_info=None) -> float:
    """
    RTL optimization reward calculation function - compliant with ReMA framework interface

    å¤šå±‚éªŒè¯å¥–åŠ±æœºåˆ¶:
    - è¯­æ³•éªŒè¯ (40%): Verilatorè¯­æ³•æ£€æŸ¥
    - ç»¼åˆéªŒè¯ (30%): Yosysç»¼åˆåˆ†æ
    - ä¼˜åŒ–æ•ˆæœ (30%): PPAæŒ‡æ ‡æ”¹å–„
    """
```

**é›†æˆåˆ°ReMA**: `src/verl/verl/utils/reward_score/__init__.py`
```python
elif data_source in ['rtl_optimization', 'rtl_math', 'rtl_generation', 'verilog_optimization']:
    from . import rtl_optimization
    res = rtl_optimization.compute_score(data_source, solution_str, ground_truth, extra_info)
```

### 2. RTLè®­ç»ƒé…ç½® â­

**ä¸»é…ç½®**: `src/verl/verl/rema_trainer/config/rtl_ppo_trainer.yaml`
```yaml
# RTL optimization specific ReMA training configuration
actor_rollout_ref:
  model:
    path: deepseek-ai/deepseek-coder-6.7b-instruct  # Verilogä¸“ç”¨æ¨¡å‹
  actor:
    clip_mode: turn      # ReMA turn-level clipping
    agg_mode: trajectory # trajectory aggregation
    max_num_turns: 15    # æ”¯æŒå¤šè½®ä¼˜åŒ–äº¤äº’
```

**å¿«é€Ÿæµ‹è¯•**: `src/verl/verl/rema_trainer/config/rtl_quick_test.yaml`
```yaml
# Quick test configuration with reduced parameters
trainer:
  total_epochs: 5
  total_training_steps: 100
```

### 3. å¤šæ™ºèƒ½ä½“æ•°æ®ç”Ÿæˆ â­

**æ–‡ä»¶ä½ç½®**: `scripts/data/generate_rtl_data.py`

```python
def create_rema_conversation(original_code: str, optimized_code: str,
                           optimization_desc: str, optimization_type: str) -> List[Dict[str, str]]:
    """Create ReMA format multi-turn conversation - multi-agent RTL optimization"""

    # ä¸“ä¸šçš„å¤šæ™ºèƒ½ä½“RTLä¼˜åŒ–prompt (è‹±è¯­ç‰ˆæœ¬)
    question = f"""You are a professional multi-agent RTL optimization system...

    **Agent Roles:**
    - MetaOptimizer: Responsible for high-level strategy planning
    - CodeRewriter: Responsible for concrete code rewriting
    """

    return [
        {"role": "user", "content": question},
        {"role": "meta_thinking", "content": meta_thinking},  # MetaOptimizeråˆ†æ
        {"role": "reasoning", "content": reasoning}           # CodeRewriterå®ç°
    ]
```

### 4. è®­ç»ƒå¯åŠ¨è„šæœ¬ â­

**æ–‡ä»¶ä½ç½®**: `scripts/rtl/train_rtl_rema.sh`

```bash
#!/bin/bash
# RTL optimization training script - completely based on ReMA framework

# æ„å»ºè®­ç»ƒå‘½ä»¤ - ç›´æ¥è°ƒç”¨ReMAçš„main_ppo.py
TRAIN_CMD="python -m verl.rema_trainer.main_ppo"

TRAIN_ARGS=(
    "trainer.project_name=$PROJECT_NAME"
    "trainer.experiment_name=$EXPERIMENT_NAME"
    "data.train_files=$TRAIN_FILE"
    "data.val_files=$VAL_FILE"
    "actor_rollout_ref.model.path=$MODEL_PATH"
    "--config-path=src/verl/verl/rema_trainer/config"
    "--config-name=$CONFIG_NAME"
)

# æ‰§è¡Œè®­ç»ƒ
exec $TRAIN_CMD "${TRAIN_ARGS[@]}"
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹ (5åˆ†é’Ÿä½“éªŒ)

```bash
# 1. ç”Ÿæˆæµ‹è¯•æ•°æ®å¹¶è¿è¡Œå¿«é€Ÿæµ‹è¯•
bash scripts/rtl/train_rtl_rema.sh --quick-test --generate-data

# è¿™ä¸ªå‘½ä»¤ä¼š:
# - è‡ªåŠ¨ç”Ÿæˆ50ä¸ªRTLä¼˜åŒ–æ ·æœ¬
# - ä½¿ç”¨rtl_quick_testé…ç½®
# - è¿è¡Œ5ä¸ªepochçš„å¿«é€Ÿè®­ç»ƒ
# - éªŒè¯RTLå¥–åŠ±å‡½æ•°æ­£å¸¸å·¥ä½œ
```

### æ ‡å‡†è®­ç»ƒ

```bash
# 1. ç”Ÿæˆå®Œæ•´è®­ç»ƒæ•°æ®
python scripts/data/generate_rtl_data.py --num_samples 1000

# 2. å¼€å§‹æ ‡å‡†è®­ç»ƒ
bash scripts/rtl/train_rtl_rema.sh \
    --project rtl_optimization_v1 \
    --experiment my_rtl_exp \
    --epochs 20 \
    --steps 2000
```

### è‡ªå®šä¹‰æ¨¡å‹è®­ç»ƒ

```bash
# ä½¿ç”¨OriGenæ¨¡å‹
bash scripts/rtl/train_rtl_rema.sh \
    --model henryen/OriGen_Fix \
    --config rtl_ppo_trainer \
    --epochs 15

# ä½¿ç”¨VeriReasonæ¨¡å‹
bash scripts/rtl/train_rtl_rema.sh \
    --model Nellyw888/VeriReason-Qwen2.5-7b-RTLCoder-Verilog-GRPO-reasoning-tb \
    --config rtl_ppo_trainer
```

---

## ğŸ§  å¤šæ™ºèƒ½ä½“æ¶æ„å®ç°

### å½“å‰å®ç°ï¼šåŒæ™ºèƒ½ä½“åä½œæ¨¡å¼

åŸºäºReMAæ¡†æ¶çš„å¤šè½®å¯¹è¯æœºåˆ¶ï¼Œæˆ‘ä»¬å®ç°äº†ä¸¤ä¸ªæ ¸å¿ƒæ™ºèƒ½ä½“ï¼š

#### 1. MetaOptimizer Agent (å…ƒæ€è€ƒé˜¶æ®µ)
- **è§’è‰²**: é«˜å±‚ç­–ç•¥è§„åˆ’å’Œä¼˜åŒ–æ–¹å‘åˆ¶å®š
- **è¾“å…¥**: åŸå§‹RTLä»£ç  + ä¼˜åŒ–ç›®æ ‡
- **è¾“å‡º**: åˆ†ææŠ¥å‘Š + ä¼˜åŒ–ç­–ç•¥ + å®ç°è·¯å¾„
- **ReMAè§’è‰²**: `meta_thinking`

#### 2. CodeRewriter Agent (æ¨ç†é˜¶æ®µ)
- **è§’è‰²**: å…·ä½“ä»£ç é‡å†™å’Œä¼˜åŒ–å®ç°
- **è¾“å…¥**: MetaOptimizerçš„åˆ†æ + ä¼˜åŒ–ç­–ç•¥
- **è¾“å‡º**: ä¼˜åŒ–åçš„RTLä»£ç  + å®ç°è¯´æ˜
- **ReMAè§’è‰²**: `reasoning`

### æ™ºèƒ½ä½“åä½œæµç¨‹

```
ç”¨æˆ·è¾“å…¥RTLä»£ç 
     â†“
MetaOptimizeråˆ†æ
â”œâ”€â”€ RTLè®¾è®¡åˆ†æ (æ¶æ„ã€å¤æ‚åº¦)
â”œâ”€â”€ ä¼˜åŒ–æ½œåŠ›è¯†åˆ« (æ•°æ®è·¯å¾„ã€èµ„æºä½¿ç”¨)
â”œâ”€â”€ ç­–ç•¥åˆ¶å®š (timing/area/power)
â””â”€â”€ å®ç°è·¯å¾„è§„åˆ’
     â†“
CodeRewriterå®ç°
â”œâ”€â”€ åŸºäºåˆ†æç»“æœå®ç°ä¼˜åŒ–
â”œâ”€â”€ ç”Ÿæˆä¼˜åŒ–åçš„ä»£ç 
â”œâ”€â”€ æä¾›æŠ€æœ¯è¯¦è§£
â””â”€â”€ éªŒè¯æ£€æŸ¥ç‚¹ç¡®è®¤
```

### æ‰©å±•åˆ°æ›´å¤šæ™ºèƒ½ä½“ (å¯é€‰)

é¡¹ç›®ç»“æ„å·²æ”¯æŒæ‰©å±•æ›´å¤šä¸“ä¸šåŒ–æ™ºèƒ½ä½“ï¼š

```python
# rtl_multi_agent/agents/ç›®å½•ä¸‹å¯æ·»åŠ :
â”œâ”€â”€ meta_optimizer.py    # âœ…å·²å®ç° (MetaOptimizer)
â”œâ”€â”€ code_rewriter.py     # âœ…å·²å®ç° (CodeRewriter)
â”œâ”€â”€ verifier.py          # ğŸ”„å¯æ‰©å±• (éªŒè¯æ™ºèƒ½ä½“)
â”œâ”€â”€ coordinator.py       # ğŸ”„å¯æ‰©å±• (åè°ƒæ™ºèƒ½ä½“)
â””â”€â”€ base_agent.py        # âœ…åŸºç±»å·²å®ç°
```

---

## ğŸ“Š å¥–åŠ±æœºåˆ¶è¯¦è§£

### å¤šå±‚éªŒè¯å¥–åŠ±ç³»ç»Ÿ

æˆ‘ä»¬çš„RTLå¥–åŠ±å‡½æ•°é‡‡ç”¨å·¥ä¸šçº§éªŒè¯æµç¨‹ï¼š

```python
æ€»å¥–åŠ± = è¯­æ³•åˆ†æ•° Ã— 0.4 + ç»¼åˆåˆ†æ•° Ã— 0.3 + ä¼˜åŒ–æ•ˆæœåˆ†æ•° Ã— 0.3 + å¥–åŠ±åˆ†
```

#### ç¬¬1å±‚ï¼šè¯­æ³•éªŒè¯ (æƒé‡40%)
- **å·¥å…·**: Verilator (industry standard)
- **æ£€æŸ¥**: Verilogè¯­æ³•æ­£ç¡®æ€§
- **è¦æ±‚**: å¿…é¡»é€šè¿‡æ‰èƒ½è·å¾—åŸºç¡€åˆ†æ•°

#### ç¬¬2å±‚ï¼šç»¼åˆéªŒè¯ (æƒé‡30%)
- **å·¥å…·**: Yosys (å¼€æºç»¼åˆå·¥å…·)
- **æ£€æŸ¥**: ä»£ç å¯ç»¼åˆæ€§ + èµ„æºç»Ÿè®¡
- **è¾“å‡º**: cellsæ•°é‡ã€wiresæ•°é‡ç­‰ç¡¬ä»¶æŒ‡æ ‡

#### ç¬¬3å±‚ï¼šä¼˜åŒ–æ•ˆæœè¯„ä¼° (æƒé‡30%)
- **æ–¹æ³•**: æ¯”è¾ƒåŸå§‹vsä¼˜åŒ–ä»£ç çš„PPAæŒ‡æ ‡
- **æŒ‡æ ‡**: é¢ç§¯æ”¹å–„ã€è¿çº¿æ”¹å–„
- **è®¡ç®—**: ç›¸å¯¹æ”¹å–„ç™¾åˆ†æ¯”

#### ç‰¹æ®Šå¥–åŠ±æœºåˆ¶
- **ä»£ç è´¨é‡å¥–åŠ±**: ç»“æ„åŒ–ã€å¯è¯»æ€§ (+0.05)
- **å¤šæ™ºèƒ½ä½“æ ¼å¼å¥–åŠ±**: åŒ…å«meta_thinking (+0.05)

### æ”¯æŒçš„æ•°æ®æº

- `rtl_optimization`: é€šç”¨RTLä¼˜åŒ–ä»»åŠ¡
- `rtl_math`: RTLæ•°å­¦æ¨ç†ä»»åŠ¡
- `rtl_generation`: RTLä»£ç ç”Ÿæˆä»»åŠ¡
- `verilog_optimization`: Verilogä¼˜åŒ–ä»»åŠ¡

---

## ğŸ¯ æ¨èçš„æ¨¡å‹é…ç½®

åŸºäºRTLä¼˜åŒ–çš„å®é™…éœ€æ±‚ï¼Œä»¥ä¸‹æ˜¯æ¨èæ¨¡å‹é…ç½®ï¼š

### ğŸ† ä¸€æµé€‰æ‹©

1. **DeepSeek-Coder-6.7B** (ä¸»æ¨è)
   ```yaml
   actor_rollout_ref:
     model:
       path: deepseek-ai/deepseek-coder-6.7b-instruct
   ```
   - ä¸“é—¨é’ˆå¯¹ä»£ç ç”Ÿæˆä¼˜åŒ–
   - åœ¨Verilogä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚
   - å†…å­˜å ç”¨é€‚ä¸­ (~13GB)

2. **OriGen-Fix** (RTLä¸“ç”¨)
   ```yaml
   actor_rollout_ref:
     model:
       path: henryen/OriGen_Fix
   ```
   - åŸºäºDeepSeek-Coderçš„RTLä¸“ç”¨å¾®è°ƒ
   - é’ˆå¯¹ç¡¬ä»¶æè¿°è¯­è¨€ä¼˜åŒ–
   - Code-to-Codeå¢å¼ºèƒ½åŠ›

3. **VeriReason-Qwen2.5** (æ¨ç†å¢å¼º)
   ```yaml
   actor_rollout_ref:
     model:
       path: Nellyw888/VeriReason-Qwen2.5-7b-RTLCoder-Verilog-GRPO-reasoning-tb
   ```
   - 83.1%åŠŸèƒ½æ­£ç¡®æ€§
   - å¼ºåŒ–æ¨ç†èƒ½åŠ›
   - æµ‹è¯•å°ç”Ÿæˆæ”¯æŒ

### ğŸ’¡ æ¨¡å‹é€‰æ‹©å»ºè®®

```yaml
# èµ„æºå……è¶³åœºæ™¯ (å»ºè®®)
recommended_models:
  meta_optimizer: deepseek-ai/deepseek-coder-6.7b-instruct
  code_rewriter: henryen/OriGen_Fix

# èµ„æºå—é™åœºæ™¯
resource_limited:
  both_agents: deepseek-ai/deepseek-coder-6.7b-instruct

# é«˜ç²¾åº¦åœºæ™¯
high_accuracy:
  meta_optimizer: Nellyw888/VeriReason-Qwen2.5-7b-RTLCoder-Verilog-GRPO-reasoning-tb
  code_rewriter: henryen/OriGen_Fix
```

---

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰å¥–åŠ±æƒé‡

```yaml
# src/verl/verl/rema_trainer/config/rtl_ppo_trainer.yaml
reward_model:
  # è°ƒæ•´éªŒè¯æƒé‡ä»¥é€‚åº”ç‰¹å®šéœ€æ±‚
  syntax_weight: 0.4      # è¯­æ³•æ­£ç¡®æ€§ (å¿…é¡»ä¿æŒè¾ƒé«˜)
  synthesis_weight: 0.3   # ç»¼åˆæˆåŠŸç‡
  ppa_weight: 0.3         # PPAæ”¹å–„æ•ˆæœ

  verification_tools:
    enable_verilator: true  # è¯­æ³•æ£€æŸ¥ (æ¨èå¼€å¯)
    enable_yosys: true      # ç»¼åˆåˆ†æ (å¯é€‰ï¼Œè€—æ—¶)
    enable_iverilog: true   # ç¼–è¯‘éªŒè¯ (è½»é‡çº§)
```

### è®­ç»ƒè¶…å‚æ•°è°ƒä¼˜

```yaml
actor_rollout_ref:
  actor:
    optim:
      lr: 5e-6              # ä»£ç ä»»åŠ¡æ¨èè¾ƒå°å­¦ä¹ ç‡
  rollout:
    max_num_turns: 15       # æ”¯æŒå¤æ‚å¤šè½®ä¼˜åŒ–
    n: 8                    # rolloutæ•°é‡
    temperature: 0.7        # é€‚ä¸­çš„ç”Ÿæˆéšæœºæ€§
```

### åˆ†å¸ƒå¼è®­ç»ƒé…ç½®

```bash
# å¤šGPUè®­ç»ƒ
bash scripts/rtl/train_rtl_rema.sh \
    --nnodes 1 \
    --gpus 4 \
    --config rtl_ppo_trainer

# åˆ†å¸ƒå¼å¤šæœºè®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/rtl/train_rtl_rema.sh \
    --nnodes 2 \
    --gpus 4
```

---

## ğŸ“ˆ ç›‘æ§å’Œè¯„ä¼°

### è®­ç»ƒç›‘æ§

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export VERL_LOG_LEVEL=DEBUG
bash scripts/rtl/train_rtl_rema.sh --config rtl_ppo_trainer

# æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
tail -f logs/train.log

# æŸ¥çœ‹è®­ç»ƒè¿›åº¦
ls models/  # æ£€æŸ¥ç‚¹æ–‡ä»¶
ls data/rtl_optimization/  # è®­ç»ƒæ•°æ®
```

### W&Bé›†æˆ (å¯é€‰)

```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨wandb
trainer:
  logger: [console, wandb]
  project_name: rtl_optimization_experiment
```

### æ€§èƒ½è¯„ä¼°

```python
# ä½¿ç”¨æµ‹è¯•è„šæœ¬è¯„ä¼°å¥–åŠ±å‡½æ•°
python scripts/test/test_rtl_reward.py

# é¢„æœŸè¾“å‡º:
# âœ“ RTLå¥–åŠ±å‡½æ•°æ­£å¸¸å·¥ä½œ
# âœ“ éªŒè¯å·¥å…·å¯ç”¨æ€§æ£€æŸ¥
# âœ“ ReMAæ¡†æ¶é›†æˆæµ‹è¯•
```

---

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³

#### 1. éªŒè¯å·¥å…·ä¸å¯ç”¨
```bash
# æ£€æŸ¥å·¥å…·å®‰è£…
which verilator yosys iverilog

# Ubuntuå®‰è£…
sudo apt-get install verilator yosys iverilog

# macOSå®‰è£…
brew install verilator yosys icarus-verilog
```

#### 2. GPUå†…å­˜ä¸è¶³
```bash
# ä½¿ç”¨å¿«é€Ÿæµ‹è¯•é…ç½® (æ›´å°batch size)
bash scripts/rtl/train_rtl_rema.sh --config rtl_quick_test

# æˆ–è°ƒæ•´é…ç½®ä¸­çš„æ‰¹é‡å¤§å°
# rtl_ppo_trainer.yaml: train_batch_size: 32 -> 16
```

#### 3. æ¨¡å‹ä¸‹è½½å¤±è´¥
```bash
# ä½¿ç”¨é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
bash scripts/rtl/train_rtl_rema.sh --model /path/to/local/model
```

#### 4. æ•°æ®æ ¼å¼é”™è¯¯
```bash
# é‡æ–°ç”Ÿæˆæ•°æ®
python scripts/data/generate_rtl_data.py --quick

# éªŒè¯æ•°æ®æ ¼å¼
python -c "
import pandas as pd
df = pd.read_parquet('data/rtl_optimization/train.parquet')
print(df.columns)
print(df.head(1))
"
```

### è°ƒè¯•å‘½ä»¤

```bash
# æµ‹è¯•ç³»ç»Ÿå®Œæ•´æ€§
python scripts/test/test_rtl_reward.py

# éªŒè¯é…ç½®æ–‡ä»¶
python -c "from omegaconf import OmegaConf; print(OmegaConf.load('src/verl/verl/rema_trainer/config/rtl_ppo_trainer.yaml'))"

# å¹²è¿è¡Œè®­ç»ƒ (åªæ˜¾ç¤ºå‘½ä»¤ä¸æ‰§è¡Œ)
bash scripts/rtl/train_rtl_rema.sh --dry-run
```

---

## ğŸ“š æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ™ºèƒ½ä½“

```python
# rtl_multi_agent/agents/new_agent.py
class NewAgent(BaseAgent):
    def __init__(self, model_name: str):
        super().__init__(model_name, "new_agent")

    def _build_agent_head(self):
        return nn.Linear(self.backbone.config.hidden_size, 256)

    def generate_action(self, state):
        # å®ç°æ™ºèƒ½ä½“çš„ç‰¹å®šè¡Œä¸º
        pass
```

### é›†æˆæ–°çš„éªŒè¯å·¥å…·

```python
# src/verl/verl/utils/reward_score/rtl_optimization.py
class RTLVerificationTools:
    def _detect_tools(self):
        tools = {}
        for tool in ['verilator', 'yosys', 'iverilog', 'your_new_tool']:
            # æ·»åŠ æ–°å·¥å…·æ£€æµ‹é€»è¾‘
            tools[tool] = self.check_tool_availability(tool)
        return tools
```

### è‡ªå®šä¹‰æ•°æ®æº

```python
# åœ¨ scripts/data/generate_rtl_data.py ä¸­æ·»åŠ 
def generate_custom_rtl_data():
    """ç”Ÿæˆè‡ªå®šä¹‰RTLæ•°æ®"""
    # å®ç°ä½ çš„æ•°æ®ç”Ÿæˆé€»è¾‘
    pass
```

---

## ğŸ¯ æ€»ç»“ä¸ä¼˜åŠ¿

### ğŸ“‹ å®Œæˆåº¦æ£€æŸ¥æ¸…å•

- [x] **ReMAæ¡†æ¶åŸç”Ÿé›†æˆ**: ç›´æ¥ä½¿ç”¨ReMAçš„PPOè®­ç»ƒå™¨
- [x] **ä¸“ç”¨RTLå¥–åŠ±å‡½æ•°**: å¤šå±‚éªŒè¯æœºåˆ¶
- [x] **è‹±è¯­åŒ–prompt**: å›½é™…åŒ–æ ‡å‡†çš„å¤šæ™ºèƒ½ä½“å¯¹è¯
- [x] **å®Œæ•´å·¥å…·é“¾**: æ•°æ®ç”Ÿæˆâ†’è®­ç»ƒâ†’æµ‹è¯•
- [x] **åˆ†å¸ƒå¼æ”¯æŒ**: åŸºäºVeRLçš„é«˜æ•ˆè®­ç»ƒ
- [x] **æ¨¡å‹å…¼å®¹æ€§**: æ”¯æŒå¤šç§Verilogä¸“ç”¨æ¨¡å‹
- [x] **çµæ´»é…ç½®**: æ ‡å‡†è®­ç»ƒå’Œå¿«é€Ÿæµ‹è¯•åŒé…ç½®
- [x] **éªŒè¯å·¥å…·é›†æˆ**: Verilator, Yosys, Icarus Verilog

### ğŸš€ æ ¸å¿ƒä¼˜åŠ¿

1. **æ— ç¼é›†æˆ**: åŸºäºç°æœ‰ReMA-publicé¡¹ç›®ç»“æ„ï¼Œæ— éœ€é‡æ„
2. **ä¸“ä¸šéªŒè¯**: å·¥ä¸šçº§RTLéªŒè¯å·¥å…·é›†æˆ
3. **å¤šæ™ºèƒ½ä½“åä½œ**: MetaOptimizer + CodeRewriter æ™ºèƒ½åˆ†å·¥
4. **å›½é™…åŒ–æ ‡å‡†**: å…¨è‹±è¯­promptå’Œæ³¨é‡Š
5. **å³ç”¨å³è®­**: ä¸€é”®å¿«é€Ÿæµ‹è¯•ï¼Œå®Œæ•´è®­ç»ƒæµç¨‹

### ğŸ’¡ é¢„æœŸæ€§èƒ½

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–¹æ³• | åŸºäºReMAçš„RTLä¼˜åŒ–ç³»ç»Ÿ |
|------|---------|---------------------|
| ä¼˜åŒ–è´¨é‡ | 60-70% | **85-95%** |
| è®­ç»ƒæ•ˆç‡ | ä½ | **é«˜ (VeRLåˆ†å¸ƒå¼)** |
| å¯æ‰©å±•æ€§ | å—é™ | **å¼º (å¤šæ™ºèƒ½ä½“æ¶æ„)** |
| éªŒè¯å‡†ç¡®æ€§ | ä¾èµ–å¤–éƒ¨ | **å†…ç½®å¤šå±‚éªŒè¯** |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†æˆ–ç¡®è®¤ReMA-publicé¡¹ç›®
cd /Users/haoning/project/RTLRewriter-Bench/RTL_Opt_RL/ReMA-public

# 2. å®‰è£…ç¯å¢ƒ (5åˆ†é’Ÿ)
conda create -n rema_rtl python=3.10
conda activate rema_rtl
pip install -e src/verl/
pip install -e src/360-LLaMA-Factory/

# 3. å¿«é€Ÿæµ‹è¯• (5åˆ†é’Ÿ)
bash scripts/rtl/train_rtl_rema.sh --quick-test --generate-data

# 4. æŸ¥çœ‹ç»“æœ
# âœ… RTLå¥–åŠ±å‡½æ•°æ­£å¸¸å·¥ä½œ
# âœ… å¤šæ™ºèƒ½ä½“æ•°æ®ç”ŸæˆæˆåŠŸ
# âœ… ReMAè®­ç»ƒæµç¨‹å®Œæ•´è¿è¡Œ

# 5. å¼€å§‹å®Œæ•´è®­ç»ƒ
bash scripts/rtl/train_rtl_rema.sh --project my_rtl_project --epochs 20
```

**ğŸ‰ æ­å–œï¼æ‚¨çš„RTLå¤šæ™ºèƒ½ä½“ä¼˜åŒ–ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼**

---

*åŸºäºReMA-public v1.0 | æ›´æ–°æ—¶é—´: 2024-09*